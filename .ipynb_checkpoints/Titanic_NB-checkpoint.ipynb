{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processing\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "\n",
    "# selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import lightgbm as lgb\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score, plot_roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from Titanic_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'text.color' : \"k\",\n",
    "                     'axes.labelcolor' : \"w\",\n",
    "                     'xtick.color' : \"w\",\n",
    "                     'ytick.color' : \"w\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to be able to reproduce features from the train to the test,\n",
    "so we'll create a custom sklean preprocessor transformer object.\n",
    "[inspiration](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    A preprocessor transformer class for the Titanic dataset.\n",
    "    The pre-processing stages are as follows:\n",
    "    - Use title of names as a feature.\n",
    "    - Fill NaN ages by the mean age grouped by Title,\n",
    "        the highest correlating feature.\n",
    "    - parse deck level from 'Cabin' feature.\n",
    "    - drop Name, Cabin, Ticket as they were used for another feature,\n",
    "        or aren't worth the trouble.\n",
    "    - one-hot categorical data.\n",
    "    - possibly add/remove columns, based on feature exploration\n",
    "        and false analysis (added ability to define what columns to drop,\n",
    "        if they prove to be bad).\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, *y):\n",
    "        \"\"\"\n",
    "        fit the preprocessor parameters to the training data.\n",
    "        :param X: data\n",
    "        :param y: labels\n",
    "        :return: none (transformer is now fitted).\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X.reset_index(inplace=True,drop=True)\n",
    "        # create Rare title for titles without enough examples (we consider\n",
    "        # it to be unknown).\n",
    "        X['Title'] = X['Name'].apply(lambda x: x.split(',')[1].split()[0])\n",
    "        self.freq_titles = X['Title'].value_counts()\n",
    "        self.freq_titles = self.freq_titles[self.freq_titles>4].index\n",
    "        X.loc[~X['Title'].isin(self.freq_titles),'Title'] = 'Rare'\n",
    "        self.ages = X.groupby('Title')['Age'].mean()\n",
    "        splitted_cabin = X['Cabin'].dropna().str.split(pat='[\\d ]').apply(lambda x: x[0])\n",
    "        X['Cabin_Deck'] = pd.DataFrame(splitted_cabin.tolist(), index = splitted_cabin.index)\n",
    "        if(self.one_hot_cat):\n",
    "            self.ohe = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "            self.ohe.fit(X[self.cat_columns].fillna(-1).astype(str))\n",
    "        else:\n",
    "            # catboost/LightGBM\n",
    "            X[self.cat_text_cols] = X[self.cat_text_cols].fillna('nan')\n",
    "            self.ord_enc = OrdinalEncoderWithUnknown()\n",
    "            self.ord_enc.fit(X[self.cat_text_cols])\n",
    "        # addition\n",
    "        X['SibSp^2'] = X['SibSp']**2\n",
    "        self.scaler = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), ['SibSp^2','Fare','Age'])],remainder='drop')\n",
    "        self.scaler.fit(X[['SibSp^2','Fare','Age']])\n",
    "\n",
    "    def add_columns_to_drop(self,cols):\n",
    "        self.drop_columns.append(cols)\n",
    "\n",
    "    def remove_columns_to_drop(self,cols):\n",
    "        self.drop_columns.remove(cols)\n",
    "\n",
    "    def drop_try(self,df):\n",
    "        '''\n",
    "        try to drop columns in 'self.columns_to_drop'.\n",
    "        if columns doesn't exist - do nothing.\n",
    "        :param df:\n",
    "        :return:\n",
    "        '''\n",
    "        for col in self.drop_columns:\n",
    "         try:\n",
    "             df.drop(col,axis=1,inplace=True)\n",
    "         except:\n",
    "             pass\n",
    "        return df\n",
    "\n",
    "    def transform(self,X, *y):\n",
    "        \"\"\"\n",
    "        transform the test/cv data to the same format as the train data.\n",
    "        :param X: data\n",
    "        :param y: none (unused for preprocessing).\n",
    "        :return: preprocessed data.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X.reset_index(inplace=True,drop=True)\n",
    "        # ticket: just shows family relations, which is covered in parch and sibsp anyway.\n",
    "        # name: we already extracted titles\n",
    "        # cabin: we used it for deck feature, no need for it afterwards.\n",
    "        # one_hot categories all at once, drop originals afterwards.\n",
    "        # name_to_titles\n",
    "        X['Title'] = X['Name'].apply(lambda x: x.split(',')[1].split()[0])\n",
    "        X.loc[~X['Title'].isin(self.freq_titles),'Title'] = 'Rare'\n",
    "        # fill age NaNs by titles\n",
    "        # X.Age = self.age_imputer.transform(X.select_dtypes(exclude=object))[:,1]\n",
    "        X.Age = X.apply(lambda x : self.ages[x['Title']] if np.isnan(x['Age']) else x['Age'],axis=1)\n",
    "        # cabin_to_deck_no (keep NaNs as is, column is cabin_deck_-1)\n",
    "        splitted_cabin = X['Cabin'].dropna().str.split(pat='[\\d ]').apply(lambda x: x[0])\n",
    "        X['Cabin_Deck'] = pd.DataFrame(splitted_cabin.tolist(), index = splitted_cabin.index)\n",
    "        # as sex has only 2 categories, just binarize it.\n",
    "        X['Sex'] = (X['Sex']=='male').astype(int)\n",
    "        # found out from feature exploration that sibsp^2 makes a difference.\n",
    "        X['SibSp^2'] = X['SibSp']**2\n",
    "        # one_hot categories all at once, drop originals afterwards.\n",
    "        if(self.one_hot_cat):\n",
    "            ohe_array = self.ohe.transform(X[self.cat_columns].fillna(-1).astype(str))\n",
    "            ohe_df = pd.DataFrame(ohe_array, columns=self.ohe.get_feature_names(self.cat_columns))\n",
    "            X = pd.concat([X,ohe_df],axis='columns').drop(self.cat_columns,axis='columns')\n",
    "        else:\n",
    "            X[self.cat_text_cols] = self.ord_enc.transform(X[self.cat_text_cols].fillna('nan')).astype(int)\n",
    "        X[['SibSp^2','Fare','Age']] = self.scaler.transform(X[['SibSp^2','Fare','Age']])\n",
    "        X = self.drop_try(X)\n",
    "        self.features = X.columns\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self,X, *y):\n",
    "        \"\"\"\n",
    "        fit the preprocessor parameters to the training data,\n",
    "        and transform the train data as well (using tranform func)\n",
    "        :param data:\n",
    "        X_train data, to fit the data preprocessor transformer.\n",
    "        :return:\n",
    "        -------\n",
    "        X_new : ndarray array of shape (n_samples, n_features_new)\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X,y)\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    def __init__(self,one_hot_cat = True,cols_to_drop = None):\n",
    "        self.one_hot_cat = one_hot_cat\n",
    "        self.drop_columns = ['Ticket','Name','Cabin']\n",
    "        if cols_to_drop is not None :\n",
    "            self.drop_columns += cols_to_drop\n",
    "        self.cat_columns = ['Pclass','Title','SibSp','Embarked','Cabin_Deck']\n",
    "        self.cat_text_cols = ['Title','Embarked','Cabin_Deck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data and classifier setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('titanic.csv')\n",
    "data.dtypes\n",
    "X, X_test, y, y_test = train_test_split(data.drop('Survived',axis=1),\n",
    "                                        data['Survived'], test_size=0.2,\n",
    "                                        random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Value\n",
    "method: just through away everything that isn't numeric,\n",
    "fill with median (robust to outliers), One_hot categories, done.\n",
    "taken from [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline scores (cv and train): \n",
      "[0.80295464 0.77913025 0.77293771 0.71675084 0.79128683]\n",
      "0.7726120561847719\n",
      "[0.78336618 0.77082512 0.77566328 0.7901736  0.77581607]\n",
      "0.7791688508252099\n"
     ]
    }
   ],
   "source": [
    "numeric_features = ['Age', 'Fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())])\n",
    "categorical_features = ['Pclass','Sex','SibSp','Parch','Embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "# note that Col Transformer returns only the transformed columns (unless remainder param set to 'passthrough')\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "baseline_clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "scores = cross_val_score_regular(baseline_clf,X,y)\n",
    "print('baseline scores (cv and train): ',*scores,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex                int32\n",
       "Age              float64\n",
       "Parch              int64\n",
       "Fare             float64\n",
       "SibSp^2          float64\n",
       "Pclass_1         float64\n",
       "Pclass_2         float64\n",
       "Pclass_3         float64\n",
       "Title_Dr.        float64\n",
       "Title_Master.    float64\n",
       "Title_Miss.      float64\n",
       "Title_Mr.        float64\n",
       "Title_Mrs.       float64\n",
       "Title_Rare       float64\n",
       "SibSp_0          float64\n",
       "SibSp_1          float64\n",
       "SibSp_2          float64\n",
       "SibSp_3          float64\n",
       "SibSp_4          float64\n",
       "SibSp_5          float64\n",
       "SibSp_8          float64\n",
       "Embarked_-1      float64\n",
       "Embarked_C       float64\n",
       "Embarked_Q       float64\n",
       "Embarked_S       float64\n",
       "Cabin_Deck_-1    float64\n",
       "Cabin_Deck_A     float64\n",
       "Cabin_Deck_B     float64\n",
       "Cabin_Deck_C     float64\n",
       "Cabin_Deck_D     float64\n",
       "Cabin_Deck_E     float64\n",
       "Cabin_Deck_F     float64\n",
       "Cabin_Deck_G     float64\n",
       "Cabin_Deck_T     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "pp.fit(X)\n",
    "pp.transform(X).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# general structure of pipeline:\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "baseline_clf\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catboost and lightgbm\n",
    " they can handle categorical data, so we'll start off with them, as they need a special setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77517686 0.8363504  0.80639731 0.78787879 0.78927284]\n",
      "0.7990152389558791\n",
      "[0.96129287 0.94410064 0.94546348 0.95018015 0.94985876]\n",
      "0.9501791791347216\n"
     ]
    }
   ],
   "source": [
    "cb_clf = CatBoostClassifier(iterations=100,\n",
    "                            task_type=\"GPU\",\n",
    "                            devices='0=1',\n",
    "                            cat_features=['Pclass','Title','SibSp',\n",
    "                                          'Parch','Embarked','Cabin_Deck'],\n",
    "                            verbose=False)\n",
    "cb = False\n",
    "if(cb):\n",
    "    pipe = Pipeline([('pre_processing',Preprocessor(one_hot_cat=False)),('classifier',cb_clf)])\n",
    "    cv_scores = cross_val_score_regular(pipe,X,y)\n",
    "    print('catboost cv and train scores: ',*cv_scores,sep='\\n')\n",
    "else:\n",
    "    lgb_params = {'classifier__categorical_feature':['Pclass','Title','SibSp','Parch','Embarked','Cabin_Deck']}\n",
    "    lgb_clf = lgb.sklearn.LGBMClassifier()\n",
    "    pipe = Pipeline([('pre_processing',Preprocessor(one_hot_cat=False)),('classifier',lgb_clf)])\n",
    "    pipe.fit(X,y)\n",
    "    cv_scores = cross_val_score_regular(pipe,X,y)\n",
    "    print(*cv_scores,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting classifiers with hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "setting up classifiers and their parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = [\"Logistic_Regression\",\n",
    "         \"Linear_SVM\", \"RBF_SVM\",\n",
    "         \"Decision_Tree\",\n",
    "         \"Neural_Net\", \"AdaBoost\",\n",
    "         \"Random_Forest\",\"XGBoost\"]\n",
    "param_grid = {'Logistic_Regression':\n",
    "                  {'classifier__C':loguniform(1e-3, 1e3),\n",
    "                   'classifier__penalty':['l1','l2']},\n",
    "              'Linear_SVM':\n",
    "                  {'classifier__C': scipy.stats.expon(scale=100),\n",
    "                    'classifier__gamma': scipy.stats.expon(scale=.1),\n",
    "                    'classifier__kernel': ['rbf'],\n",
    "                    'classifier__class_weight':['balanced', None]},\n",
    "              'RBF_SVM':\n",
    "                  {'classifier__C': loguniform(1e0, 1e3),\n",
    "                    'classifier__gamma': loguniform(1e-4, 1e-3),\n",
    "                    'classifier__kernel': ['rbf']},\n",
    "              'Decision_Tree':\n",
    "                  {\"classifier__max_depth\": np.linspace(2,10,5),\n",
    "                     \"classifier__max_features\": range(1,9),\n",
    "                     \"classifier__min_samples_leaf\": range(3,9),\n",
    "                     \"classifier__criterion\": [\"gini\", \"entropy\"]},\n",
    "              'Neural_Net':{\n",
    "                  'classifier__solver': ['lbfgs','adam'],\n",
    "                  'classifier__max_iter': [500,1000,1500],\n",
    "                  'classifier__alpha': 10.0 ** -np.arange(1, 7),\n",
    "                  'classifier__hidden_layer_sizes':np.arange(5, 12)\n",
    "              },\n",
    "              'AdaBoost':{\n",
    "                  'classifier__n_estimators': [100,200,500],\n",
    "                  'classifier__learning_rate' : loguniform(1e-2, 1e0),\n",
    "                  'classifier__base_estimator__max_depth' : [4,5,6,7,8]\n",
    "              },\n",
    "              'Random_Forest':{\n",
    "                  'classifier__n_estimators': [200, 500],\n",
    "                  'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                  'classifier__max_depth' : [4,5,6,7,8],\n",
    "                  'classifier__criterion' :['gini', 'entropy']},\n",
    "              'XGBoost':{\n",
    "                  'classifier__learning_rate':loguniform(1e-2, 1e0),\n",
    "                  'classifier__gamma':[0,1,5],\n",
    "                  'classifier__n_estimators': [500,750,1000],\n",
    "                  'classifier__max_depth': [3,6,8],\n",
    "                  'classifier__subsample': uniform(loc=0.8,scale=0.2),\n",
    "                  'classifier__colsample_bytree': uniform(loc=0.8,scale=0.2)\n",
    "              }}\n",
    "classifiers = [\n",
    "    LogisticRegression(C=0.3,n_jobs=-1,solver='newton-cg'),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    MLPClassifier(hidden_layer_sizes=(5,3),alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=10),n_estimators=500),\n",
    "    RandomForestClassifier(max_depth=9, n_estimators=400, max_features=1),\n",
    "    xgb.XGBClassifier(max_depth=10,learning_rate =0.3,n_estimators=500,\n",
    "                      objective=\"reg:squarederror\", n_jobs = -1,random_state=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Search on each Clf (20 rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 6.72 seconds for 20 candidates parameter settings.\n",
      "Pipeline(steps=[('pre_processing', Preprocessor()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.10619108399169305, n_jobs=-1,\n",
      "                                    solver='newton-cg'))]) \n",
      "auc:  0.8625159222427083\n",
      "Linear_SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 4.21 seconds for 20 candidates parameter settings.\n",
      "Pipeline(steps=[('pre_processing', Preprocessor()),\n",
      "                ('classifier',\n",
      "                 SVC(C=16.493994305284705, gamma=0.0190010899985659))]) \n",
      "auc:  0.8624328267838444\n",
      "RBF_SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 3.86 seconds for 20 candidates parameter settings.\n",
      "Pipeline(steps=[('pre_processing', Preprocessor()),\n",
      "                ('classifier',\n",
      "                 SVC(C=25.306063066623494, gamma=0.00012337949527837963))]) \n",
      "auc:  0.8566019667739511\n",
      "Decision_Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 3.34 seconds for 20 candidates parameter settings.\n",
      "Pipeline(steps=[('pre_processing', Preprocessor()),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(max_depth=8.0, max_features=8,\n",
      "                                        min_samples_leaf=6))]) \n",
      "auc:  0.8290599175987134\n",
      "Neural_Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "C:\\Users\\Ariel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "random_searches = {}\n",
    "for (name,clf) in zip(names, classifiers):\n",
    "    pipeclf = Pipeline([('pre_processing',Preprocessor()),('classifier',clf)])\n",
    "    print(name)\n",
    "    n_iter_search = 20\n",
    "    random_searches[name] = RandomizedSearchCV(pipeclf,param_grid[name],\n",
    "                                               n_jobs=-1,scoring='roc_auc',\n",
    "                                               n_iter=n_iter_search,\n",
    "                                               return_train_score=True)\n",
    "    start = time()\n",
    "    random_search = random_searches[name]\n",
    "    random_search.fit(X, y)\n",
    "    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "          \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "    print(random_search.best_estimator_,'\\nauc: ',random_search.best_score_)\n",
    "    scores.append((name,random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# False analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save searches.\n",
    "from joblib import dump, load\n",
    "dump(random_searches, 'random_searches.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_searches = load('random_searches.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see scores of the best classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_searches_list = list(random_searches.items())\n",
    "random_searches_list.sort(key=lambda x:x[1].best_score_,reverse=True)\n",
    "for (name,rand_search) in random_searches_list[:3]:\n",
    "    print(name,'\\n')\n",
    "    print('best index: ',rand_search.best_index_)\n",
    "    for metric in rand_search.cv_results_.items():\n",
    "        print(metric[0],metric[1][rand_search.best_index_])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_clf = random_searches_list[0][1].best_estimator_\n",
    "cols = best_clf['pre_processing'].fit_transform(X).columns\n",
    "importances = best_clf['classifier'].feature_importances_\n",
    "df = pd.DataFrame(np.c_[cols,importances],columns=['Columns','Importances'])\n",
    "print(df)\n",
    "# print(list(df[df['Importances']==0]['Columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances,fp,fn = get_errors_and_features(best_clf,X,y)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that most misclassifications are fn.\n",
    "let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_clf['pre_processing'].transform(fn)['Title_Mr.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "we see that almost all fn's are men ('Mr.' to be specific). In addition,\n",
    "we see that the classifier puts a large emphasis on both title and one's Sex.\\\n",
    "Since we can assume the Titles cover one's sex (a 'Mrs.' is a female, etc),\\\n",
    "we'll try to remove the Sex, as it has a high correlation with the titles.\n",
    "to prove this, we'll check it's chi2 score with \"Mr.\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans_x = best_clf['pre_processing'].transform(X)\n",
    "chi2(trans_x['Title_Mr.'].to_frame(),trans_x['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "voalá. Insane score. let's try the same thing without Sex (as the titles\n",
    "are more precise than Sex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans_x_no_sex = trans_x.drop('Sex',axis=1)\n",
    "print('original score, with sex:\\n',\n",
    "      *cross_val_score_regular(best_clf['classifier'],trans_x,y),sep='\\n')\n",
    "print('\\n\\n\"improved\" score, without sex:\\n',\n",
    "      *cross_val_score_regular(best_clf['classifier'],trans_x_no_sex,y),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "aaaaaand it didn't work :(\n",
    "let's try removing the columns that aren't relevant to the model (i.e. got feature_importance 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [x[1] for x in feature_importances if x[0]==0]\n",
    "print('old pipe',*cross_val_score_regular(best_clf,X,y),sep='\\n')\n",
    "pipe_filtered = Pipeline([('pre_processing',Preprocessor(cols_to_drop=cols_to_drop)),\n",
    "                    ('classifier', best_clf['classifier'])])\n",
    "print('new pipe',*cross_val_score_regular(pipe_filtered,X,y),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.c_[pipe_filtered['classifier'].feature_importances_,\n",
    "      pipe_filtered['pre_processing'].get_features()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in addition, let's try creating the clf again, but incorporating the skewness of the class.\n",
    "scale_pos_weight=negatives/positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params = pipe_filtered['classifier'].get_params()\n",
    "xgb_params['scale_pos_weight'] = ((y==1).sum()/(y==0).sum()) # positive cases/neg cases.\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "old_clf = best_clf['classifier']\n",
    "skew_pipe = Pipeline([('pre_processing',pipe_filtered['pre_processing']),\n",
    "                    ('classifier', xgb.XGBClassifier(**xgb_params))])\n",
    "print('filtered pipe:',*cross_val_score_regular(pipe_filtered,X,y),sep='\\n')\n",
    "print('skewed pipe',*cross_val_score_regular(skew_pipe,X,y),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "didn't work. reverting back to filtered pipe.\n",
    "finally, we should try dealing with the fact that the data is skewed\n",
    "by stratifying the initial split maybe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_strat,X_test_strat,y_strat,y_test_strat = train_test_split(data.drop('Survived',axis=1),\n",
    "                                        data['Survived'], test_size=0.2,\n",
    "                                        random_state=42,stratify=data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score_regular(pipe_filtered,X_strat,y_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We didn't change the model, only now we're looking at a more realistic test case.\n",
    "now, it seems we overfit the training set. we need to reduce the expressibility of the model.\n",
    "let's reduce the dimension of the model, by further removing features below a certain\n",
    "threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop_2 = [x[1] for x in np.c_[pipe_filtered['classifier'].feature_importances_,\n",
    "                                   pipe_filtered['pre_processing'].get_features()] if x[0]<0.01]\n",
    "pipe_filtered_2 = Pipeline([('pre_processing',Preprocessor(cols_to_drop=cols_to_drop+cols_to_drop_2)),\n",
    "                    ('classifier', best_clf['classifier'])])\n",
    "pipe_filtered_2\n",
    "print('filtered_pipe_1',*cross_val_score_regular(pipe_filtered,X,y),sep='\\n')\n",
    "print('pipe_filtered_2',*cross_val_score_regular(pipe_filtered_2,X,y),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess good enough for now.\n",
    "### visualize roc_auc score\n",
    "taken from [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "kf = StratifiedKFold()\n",
    "lr_clf_pipe = pipe_filtered_2\n",
    "for i, (train, test) in enumerate(kf.split(X, y)):\n",
    "    lr_clf_pipe.fit(X.iloc[train], y.iloc[train])\n",
    "    viz = plot_roc_curve(lr_clf_pipe, X.iloc[test], y.iloc[test],\n",
    "                         name='ROC fold {}'.format(i),\n",
    "                         lw=1, ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic example\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# the final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# final test\n",
    "best_clf = pipe_filtered_2\n",
    "best_clf.fit(X,y)\n",
    "pred = best_clf.predict(X_test)\n",
    "print('score: ',roc_auc_score(y_test,pred))\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print('fp: \\n', X_test[(y_test != pred) & (y_test == 0)])\n",
    "fn = X_test[(y_test != pred) & (y_test == 1)]\n",
    "print('\\n\\nfn: \\n', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test.dtypes\n",
    "# X.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "newbie = [1,'Shumacher, Mr. Ariel','male',21,2,4,None,30,'E3','S']\n",
    "def get_df_like_titanic(x):\n",
    "    res = X.iloc[0:0,:].copy()\n",
    "    res.append(x)\n",
    "    return res\n",
    "def will_you_survive(*params):\n",
    "    res = X.iloc[0:0,:].copy()\n",
    "    df = pd.DataFrame(np.atleast_2d(params),columns=X.columns)\n",
    "    for x in X.columns:\n",
    "        df[x]=df[x].astype(X[x].dtypes.name)\n",
    "    print(df,'\\n',df.dtypes)\n",
    "    print('will you survive?: ',best_clf.predict(df)==1,\n",
    "          '\\nwith probability:',best_clf.predict_proba(df))\n",
    "will_you_survive(*newbie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Project)",
   "language": "python",
   "name": "pycharm-1a86bc75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
